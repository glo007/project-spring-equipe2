# project-spring-equipe2
TP DataOps, Projet dâ€™Ã©quipe dans le cadre du module Spring / Scrum
# ğŸš€ Projet DataPulse - Sprint 1 (DataOps)

## ğŸ§© Contexte du projet

Le projet **DataPulse** sâ€™inscrit dans une dÃ©marche **DataOps**, visant Ã  automatiser et fiabiliser les flux de donnÃ©es dâ€™une organisation.  
Lâ€™objectif est de concevoir un pipeline complet permettant :
- Lâ€™ingestion de donnÃ©es brutes (CSV ou API),
- Leur transformation et nettoyage automatisÃ©,
- Leur stockage dans une base de donnÃ©es structurÃ©e,
- Et leur visualisation via un dashboard simple (tableau ou graphique).

Ce sprint a pour but de simuler un cycle **Scrum complet** appliquÃ© Ã  un projet DataOps.

---

## ğŸ¯ Objectifs du Sprint 1

- CrÃ©er le dÃ©pÃ´t GitHub avec gestion des issues et backlog.  
- Identifier et dÃ©crire les **User Stories (US)** principales.  
- DÃ©taillez les **tÃ¢ches techniques** Ã  rÃ©aliser.  
- Estimer la charge de travail avec des **Story Points (Fibonacci)**.  
- Planifier le sprint et rÃ©partir les rÃ´les.

---

## ğŸ‘¥ Organisation Scrum de lâ€™Ã©quipe

| RÃ´le | Membre | ResponsabilitÃ©s |
|------|---------|----------------|
| **Product Owner** | Arcy | DÃ©finit les prioritÃ©s du backlog, veille Ã  la valeur mÃ©tier. |
| **Scrum Master** | Ann-Jireh | Garant de la mÃ©thode agile, facilite la communication. |
| **Data Engineer** | Bryan | Met en place les pipelines et transformations de donnÃ©es. |
| **Data Analyst** | Mohammed | Analyse les donnÃ©es et conÃ§oit le tableau de bord final. |
| **DevOps Engineer** | Mhammed | Automatise le dÃ©ploiement et la supervision des flux. |

---

## ğŸ—‚ï¸ Organisation GitHub Project

**Nom du projet GitHub :** `Sprint 1 - DataOps`  
**Type :** Kanban board  
**Colonnes :**  
- ğŸ“‹ Backlog (To do)  
- âš™ï¸ In Progress  
- ğŸ” In Review  
- âœ… Done  

Chaque **issue** correspond Ã  une **User Story (US)**.  
Les **tÃ¢ches techniques** seront listÃ©es dans la description de chaque issue.

---

## ğŸ“‹ Product Backlog â€” User Stories (US)

| ID | User Story | Description | CritÃ¨res dâ€™acceptation | Estimation (SP) |
|----|-------------|--------------|-------------------------|-----------------|
| **US1** | Ingestion automatisÃ©e des donnÃ©es | En tant que Data Engineer, je veux automatiser la rÃ©cupÃ©ration de fichiers CSV pour alimenter la base de donnÃ©es. | Le pipeline doit pouvoir lire automatiquement les fichiers dâ€™un dossier et les insÃ©rer en base. | 5 |
| **US2** | Nettoyage et transformation des donnÃ©es | En tant que Data Analyst, je veux transformer les donnÃ©es pour les rendre exploitables (valeurs manquantes, formatage, etc.). | Les donnÃ©es nettoyÃ©es doivent Ãªtre stockÃ©es dans une table "clean_data". | 3 |
| **US3** | Stockage structurÃ© | En tant que DevOps Engineer, je veux stocker les donnÃ©es transformÃ©es dans une base PostgreSQL. | Les donnÃ©es doivent Ãªtre accessibles via SQL. | 3 |
| **US4** | Visualisation des donnÃ©es | En tant que Data Analyst, je veux afficher les rÃ©sultats dans un dashboard (via Python/Streamlit). | Le dashboard doit montrer les indicateurs principaux. | 8 |
| **US5** | Supervision du pipeline | En tant que DevOps Engineer, je veux mettre en place des logs et une supervision des erreurs. | Le systÃ¨me doit notifier en cas dâ€™Ã©chec. | 5 |

---

## ğŸ§© DÃ©tail des tÃ¢ches techniques (Sprint Backlog)

| User Story | TÃ¢che | Responsable | Estimation (SP) | Livrable attendu |
|-------------|-------|--------------|------------------|------------------|
| US1 | CrÃ©er le script dâ€™ingestion (Python) | Axel | 3 | Script dâ€™import automatique |
| US1 | Automatiser via un cron job ou scheduler | Kevin | 2 | Script exÃ©cutÃ© toutes les heures |
| US2 | CrÃ©er le script de nettoyage (Pandas) | Sarah | 3 | DonnÃ©es nettoyÃ©es |
| US3 | Concevoir le schÃ©ma PostgreSQL | Axel | 3 | Table structurÃ©e |
| US3 | Connecter Python Ã  PostgreSQL | Kevin | 2 | Connexion validÃ©e |
| US4 | CrÃ©er un dashboard Streamlit | Sarah | 5 | Tableau de bord visuel |
| US5 | Ajouter des logs (logging Python) | Kevin | 3 | Fichier de logs |
| US5 | Mettre en place un rapport dâ€™erreurs (email/console) | Ann-Jireh | 2 | Notification dâ€™alerte |

---

## ğŸ• Estimation des Story Points (suite de Fibonacci)

| ComplexitÃ© | Valeur | Description |
|-------------|--------|-------------|
| TrÃ¨s simple | 1 | Configuration rapide ou correction mineure |
| Simple | 2 | Petite fonctionnalitÃ© (script, test, doc) |
| Moyenne | 3 | Script avec dÃ©pendances ou API |
| Complexe | 5 | Pipeline complet ou dashboard |
| TrÃ¨s complexe | 8 | IntÃ©gration multiple ou automatisation avancÃ©e |

---

## ğŸ—“ï¸ Planification du Sprint 1

| Sprint | DurÃ©e | Objectifs principaux |
|--------|--------|----------------------|
| Sprint 1 | 1 semaine | Mettre en place lâ€™ingestion, le nettoyage et le stockage initial des donnÃ©es. |

**Livrables attendus :**
- Scripts fonctionnels (Python)
- Base PostgreSQL connectÃ©e
- Dashboard Streamlit minimal
- Documentation du sprint

---

## ğŸŒ€ Ã‰vÃ©nements Scrum simulÃ©s

| Ã‰vÃ©nement | Objectif | DurÃ©e indicative |
|------------|-----------|------------------|
| **Sprint Planning** | Choisir les US rÃ©alisables en 1 semaine | 1h |
| **Daily Scrum** | Suivi quotidien des avancÃ©es | 15 min |
| **Sprint Review** | PrÃ©senter les livrables | 30 min |
| **Sprint Retrospective** | Identifier les amÃ©liorations | 30 min |

---

## ğŸ“ˆ Bilan du Sprint 1

| Question | RÃ©ponse |
|-----------|----------|
| âœ… Ce qui a bien fonctionnÃ© | Bonne communication et rÃ©partition des rÃ´les clairs. |
| âš ï¸ Ã€ amÃ©liorer | Estimations parfois sous-Ã©valuÃ©es. |
| ğŸš€ Pour le prochain sprint | Ajouter des tests unitaires et un monitoring plus dÃ©taillÃ©. |

---

## ğŸ“š Documentation complÃ©mentaire

- Dossier `/docs` : guide dâ€™installation, architecture technique, scripts Python.
- Fichier `/scripts` : contient les scripts dâ€™ingestion et transformation.
- Fichier `/dashboard` : dashboard Streamlit ou Jupyter Notebook.

---

## ğŸ Conclusion

Le sprint 1 permet de poser les bases dâ€™un processus **DataOps automatisÃ©**, garantissant la qualitÃ©, la fiabilitÃ© et la traÃ§abilitÃ© des donnÃ©es Ã  chaque Ã©tape du pipeline.
